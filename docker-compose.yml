name: datastore

services:
  db:
    #    image: timescale/timescaledb-ha:pg15-latest
    image: kartoza/postgis:15 # Use this instead of the official image as it has an arm64 image
    ports:
      - "5433:5432"
    volumes:
      #      - ts-data:/home/postgres/pgdata/data # for timescale image
      - ts-data:/var/lib/postgresql # for postgres image
      - ./datastore/database/extra.conf:/etc/conf_settings/extra.conf:ro  # Extra Postgres configuration
      - ./datastore/database/healthcheck_postgis_uptime.sh:/healthcheck_postgis_uptime.sh:ro # for the healthcheck
    environment:
      - EXTRA_CONF_DIR=/etc/conf_settings
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=mysecretpassword
      - POSTGRES_DB=data
    shm_size: 312m
    restart: on-failure
    healthcheck:
      # HACK Due to the installation of Postgis extension the database is restarted, the healthcheck checks if the database is up for longer than specified time.
      test:
        [
          "CMD-SHELL",
          "/healthcheck_postgis_uptime.sh postgresql://postgres:mysecretpassword@localhost/data 10 second",
        ]
      interval: 5s
      timeout: 1s
      retries: 3
      start_period: 30s # Failures in 30 seconds do not mark container as unhealthy

  migrate:
    build:
      context: datastore/migrate
    environment:
      - DB_USER=postgres
      - DB_PASS=mysecretpassword
      - DB_URL=db
    depends_on:
      db:
        condition: service_healthy

  store:
    build:
      context: datastore/datastore
    ports:
      - "50050:50050"
      - "6060:6060" # for flame graphing
      - "8081:8081"
    environment:
      - PGHOST=db
      - PGPORT=5432
      - DYNAMICTIME=$DYNAMICTIME
      - LOTIME=$LOTIME
      - HITIME=$HITIME
      - CLEANUPINTERVAL=$CLEANUPINTERVAL
      - PUTOBSLIMIT=$PUTOBSLIMIT
    restart: on-failure
    healthcheck:
      test:
        ["CMD-SHELL", "exit $(/bin/grpc_health_probe -addr=localhost:50050)"]
      interval: 5s
      timeout: 1s
      retries: 15
      start_period: 60s
    depends_on:
      migrate:
        condition: service_completed_successfully

  api:
    build:
      context: api
    ports:
      - "8008:8000"
    environment:
      - DSHOST=store
      - DSPORT=50050
      - FORWARDED_ALLOW_IPS=${FORWARDED_ALLOW_IPS:-127.0.0.1}
      - GUNICORN_CMD_ARGS=--bind 0.0.0.0:8000 --workers=4 --access-logfile -
    depends_on:
      store:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:8000/health || exit 1"]
      interval: 5s
      timeout: 1s
      retries: 3
      start_period: 30s # Failures in 30 seconds do not mark container as unhealthy

  api-unit:
    profiles: ["test"]
    build:
      context: api
      dockerfile: unit.Dockerfile
    volumes:
      - ./api/test/output:/app/output

  ingest:
    build:
      context: ingest
    ports:
      - "8009:8001"
    network_mode: ""
    environment:
      - DSHOST=${DSHOST:-store}
      - DSPORT=${DSPORT:-50050}
      - MQTT_HOST=${MQTT_HOST}
      - MQTT_USERNAME=${MQTT_USERNAME}
      - MQTT_PASSWORD=${MQTT_PASSWORD}
      - MQTT_TLS=True
      - INGEST_LOGLEVEL
      - GUNICORN_CMD_ARGS=--bind 0.0.0.0:8001 --workers=4 --access-logfile -
    depends_on:
      store:
        condition: service_healthy

  ingest-unit:
    profiles: ["test"]
    build:
      context: ingest
      dockerfile: unit.Dockerfile
    volumes:
      - ./ingest/test/output:/app/output

  client:
    profiles: ["test"]
    build:
      context: datastore/examples/clients/python
    environment:
      - DSHOST=store
      - DSPORT=50050
    depends_on:
      store:
        condition: service_healthy

  loader:
    profiles: ["test"]
    build:
      context: datastore/data-loader
    environment:
      - DSHOST=store
      - DSPORT=50050
      - BASE_URL=http://api:8000
    depends_on:
      store:
        condition: service_healthy

  ingest-loader:
    profiles: ["test"]
    build:
      context: datastore/data-loader
      args:
        THROUGH_INGEST: "true"
    environment:
      - INGEST_URL=http://ingest:8001/json
    depends_on:
      store:
        condition: service_healthy
      ingest:
        condition: service_started

  integration:
    profiles: ["test"]
    build:
      context: datastore/integration-test
    environment:
      - DSHOST=store
      - DSPORT=50050
      - BASE_URL=http://api:8000
    depends_on:
      api:
        condition: service_healthy

  performance:
    profiles: ["test"]
    build:
      context: datastore/load-test
    environment:
      - DSHOST=store
      - DSPORT=50050
    volumes:
      - ./datastore/load-test/output:/load-test/output:rw
    depends_on:
      store:
        condition: service_healthy

  prometheus:
    profiles: ["monitoring"]
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus

  prometheus-postgres-exporter:
    profiles: ["monitoring"]
    image: quay.io/prometheuscommunity/postgres-exporter
    environment:
      - DATA_SOURCE_URI=db:5432/data
      - DATA_SOURCE_USER=postgres
      - DATA_SOURCE_PASS=mysecretpassword
    ports:
      - "9187:9187"
    volumes:
      - ./prometheus/postgres_exporter.yml:/postgres_exporter.yml:ro
    depends_on:
      db:
        condition: service_healthy
    command: ["--collector.stat_statements", "--collector.stat_user_tables", "--collector.stat_activity_autovacuum"]

  grafana:
    profiles: ["monitoring"]
    image: grafana/grafana-oss:11.2.0
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=mysecretpassword
    volumes:
      - grafana-storage:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:rw
      - ./grafana/dashboards:/var/lib/grafana/dashboards:rw
    depends_on:
      - prometheus

volumes:
  ts-data:
  prometheus-data:
  grafana-storage:
